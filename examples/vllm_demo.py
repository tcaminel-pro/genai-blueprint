#!/usr/bin/env python3
"""
VLLM Integration Demo for GenAI-Blueprint

This script demonstrates how to use the VLLM endpoint integration with the GenAI-Blueprint framework.

Prerequisites:
1. Install vLLM: pip install vllm
2. Ensure you have a CUDA-compatible GPU (for local inference)
3. Download the model you want to use (or use HuggingFace Hub)

Usage:
    python examples/vllm_demo.py

Note: This demo shows the configuration and setup. Actual model inference requires
GPU resources and model downloads which may take significant time and storage.
"""

import os
import sys
from pathlib import Path

# Add src to path for imports
sys.path.append(str(Path(__file__).parent.parent / "src"))

from src.ai_core.llm_factory import LlmFactory, get_llm_info, get_llm


def demo_vllm_configuration():
    """Demonstrate VLLM model configuration retrieval."""
    print("üöÄ VLLM Integration Demo for GenAI-Blueprint")
    print("=" * 50)
    
    # Get all available VLLM models
    all_llms = LlmFactory.known_list()
    vllm_models = [llm for llm in all_llms if llm.provider == "vllm"]
    
    print(f"üìã Found {len(vllm_models)} VLLM models configured:")
    for model in vllm_models:
        print(f"  ‚Ä¢ {model.id}")
        print(f"    Model: {model.model}")
        print(f"    Args: {model.llm_args}")
        print()


def demo_vllm_factory():
    """Demonstrate VLLM factory usage."""
    print("üè≠ VLLM Factory Demo")
    print("-" * 30)
    
    # Example model ID (you can change this to any VLLM model from the config)
    model_id = "mpt_7b_vllm_vllm"
    
    try:
        # Get model information
        info = get_llm_info(model_id)
        print(f"üìä Model Info for {model_id}:")
        print(f"  Provider: {info.provider}")
        print(f"  Model: {info.model}")
        print(f"  Parameters: {info.llm_args}")
        print()
        
        # Create factory (this doesn't instantiate the actual model)
        factory = LlmFactory(llm_id=model_id)
        print(f"üè≠ Factory created successfully for {factory.short_name()}")
        print(f"  Provider: {factory.provider}")
        print()
        
        # Check if vLLM is available
        try:
            import vllm
            print("‚úÖ vLLM package is available")
            print("üìù To actually use the model, you would call:")
            print(f"   llm = get_llm(llm_id='{model_id}')")
            print("   response = llm.invoke('Your prompt here')")
            print()
            print("‚ö†Ô∏è  Note: This requires GPU resources and model download")
            
        except ImportError:
            print("‚ùå vLLM package not installed")
            print("üí° Install with: pip install vllm")
            
    except Exception as e:
        print(f"‚ùå Error: {e}")


def demo_vllm_usage_example():
    """Show example usage code."""
    print("üí° Example Usage Code")
    print("-" * 30)
    
    example_code = '''
# Example: Using VLLM with GenAI-Blueprint
from src.ai_core.llm_factory import get_llm

# Method 1: Direct model specification
llm = get_llm(llm_id="mpt_7b_vllm_vllm")

# Method 2: With custom parameters
llm = get_llm(
    llm_id="llama32_3b_vllm_vllm",
    temperature=0.8,
    max_tokens=256
)

# Method 3: Using in a chain
from langchain_core.prompts import ChatPromptTemplate

prompt = ChatPromptTemplate.from_template("Explain {topic} in simple terms.")
chain = prompt | llm

# Generate response
response = chain.invoke({"topic": "machine learning"})
print(response.content)

# Method 4: Streaming response
llm_streaming = get_llm(llm_id="mpt_7b_vllm_vllm", streaming=True)
for chunk in llm_streaming.stream("Tell me about AI"):
    print(chunk.content, end="", flush=True)
'''
    
    print(example_code)


def main():
    """Main demo function."""
    demo_vllm_configuration()
    demo_vllm_factory()
    demo_vllm_usage_example()
    
    print("üéâ VLLM integration demo completed!")
    print("üìö For more information, see:")
    print("  ‚Ä¢ LangChain VLLM docs: https://python.langchain.com/docs/integrations/llms/vllm/")
    print("  ‚Ä¢ VLLM GitHub: https://github.com/vllm-project/vllm")
    print("  ‚Ä¢ GenAI-Blueprint docs: config/providers/llm.yaml")


if __name__ == "__main__":
    main()