{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from dotenv import load_dotenv\n",
                "\n",
                "load_dotenv(verbose=True)\n",
                "\n",
                "%load_ext autoreload\n",
                "%autoreload 2\n",
                "\n",
                "!export PYTHONPATH=\":./python\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#!pip3 install -U langchain-community faiss-cpu langchain-openai tiktoken\n",
                "#!pip3 install -U giskard"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import giskard\n",
                "import pandas as pd\n",
                "from langchain.document_loaders import PyPDFLoader\n",
                "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
                "from langchain_community.document_loaders import PyPDFLoader"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare vector store (FAISS) with IPPC report\n",
                "\n",
                "from genai_tk.core.embeddings_factory import EmbeddingsFactory\n",
                "from genai_tk.core.llm_factory import get_llm\n",
                "from genai_tk.core.prompts import def_prompt\n",
                "from genai_tk.core.embeddings_store import EmbeddingsStore\n",
                "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
                "from langchain.chains.retrieval import create_retrieval_chain\n",
                "\n",
                "embeddings_store = EmbeddingsStore(\n",
                "    id=\"Chroma_in_memory\",\n",
                "    table_name_prefix=\"giskard_test\",\n",
                "    embeddings_factory=EmbeddingsFactory(),\n",
                ")\n",
                "\n",
                "DOC = \"https://www.ipcc.ch/report/ar6/syr/downloads/report/IPCC_AR6_SYR_LongerReport.pdf\"\n",
                "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100, add_start_index=True)\n",
                "documents = PyPDFLoader(DOC).load()\n",
                "texts = text_splitter.split_documents(documents)\n",
                "embeddings_store.add_documents(texts)\n",
                "\n",
                "\n",
                "# Prepare QA chain\n",
                "system_prompt = \"\"\"You are the Climate Assistant, a helpful AI assistant made by Eviden.\n",
                "Your task is to answer common questions on climate change.\n",
                "You will be given a question and relevant excerpts from the IPCC Climate Change Synthesis Report (2023).\n",
                "Please provide short and clear answers based on the provided context. Be polite and helpful.\n",
                "\n",
                "Context:\n",
                "{context}\"\"\"\n",
                "\n",
                "user_prompt = \"\"\"\n",
                "Question:\n",
                "{question}\n",
                "\n",
                "Your answer:\n",
                "\"\"\"\n",
                "\n",
                "llm = get_llm(llm_id=\"gpt_35_openai\")\n",
                "\n",
                "\n",
                "prompt = def_prompt(system=system_prompt, user=user_prompt)\n",
                "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
                "climate_qa_chain = create_retrieval_chain(embeddings_store.get().as_retriever(), question_answer_chain)\n",
                "\n",
                "# chain.invoke({\"input\": query})"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def model_predict(df: pd.DataFrame):\n",
                "    \"\"\"Wraps the LLM call in a simple Python function.\n",
                "\n",
                "    The function takes a pandas.DataFrame containing the input variables needed\n",
                "    by your model, and must return a list of the outputs (one for each row).\n",
                "    \"\"\"\n",
                "    return [climate_qa_chain.invoke({\"query\": question}) for question in df[\"question\"]]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from giskard.llm.client.openai import OpenAIClient\n",
                "\n",
                "giskard.llm.set_llm_api(\"openai\")\n",
                "oc = OpenAIClient(model=\"gpt-4-turbo-preview\")\n",
                "giskard.llm.set_default_client(oc)\n",
                "\n",
                "giskard_model = giskard.Model(\n",
                "    model=model_predict,\n",
                "    model_type=\"text_generation\",\n",
                "    name=\"Climate Change Question Answering\",\n",
                "    description=\"This model answers any question about climate change based on IPCC reports\",\n",
                "    feature_names=[\"question\"],\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "scan_results = giskard.scan(giskard_model)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "display(scan_results)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Or save it to a file\n",
                "scan_results.to_html(\"scan_results.html\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}