{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Structured Output\n",
                "\n",
                "This notebook demonstrates two methods for getting structured output from LLMs:\n",
                "\n",
                "1. **Prompt Engineering**: Including output format instructions directly in the prompt\n",
                "2. **Function Calling**: Using the model's built-in structured output capabilities\n",
                "\n",
                "We'll use joke generation as our example task to compare these approaches."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from dotenv import load_dotenv\n",
                "from rich import print\n",
                "\n",
                "load_dotenv(verbose=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Method 1: Prompt Engineering\n",
                "\n",
                "This approach uses LangChain's `PydanticOutputParser` to:\n",
                "\n",
                "1. Define our desired output structure using Pydantic\n",
                "2. Automatically generate format instructions\n",
                "3. Inject those instructions into the prompt\n",
                "\n",
                "The model then returns text that we parse into our structured format."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain.output_parsers import PydanticOutputParser\n",
                "from pydantic import BaseModel, Field"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The usual \"tell me a joke\" LLM call..."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from genai_tk.core.llm_factory import get_llm\n",
                "from genai_tk.core.prompts import def_prompt\n",
                "\n",
                "\n",
                "class Joke(BaseModel):\n",
                "    the_joke: str = Field(description=\"a good joke\")\n",
                "    explanation: str = Field(description=\"explain why it's funny in exactly 10 words\")\n",
                "    rate: float = Field(description=\"rate how the joke is funny between 0 and 5\")\n",
                "\n",
                "\n",
                "parser = PydanticOutputParser(pydantic_object=Joke)\n",
                "\n",
                "prompt_with_format = \"\"\"\n",
                "    tell me  a joke on {topic}\n",
                "    ---\n",
                "    {format_instructions}\"\"\"\n",
                "\n",
                "structured_prompt = def_prompt(user=prompt_with_format).partial(\n",
                "    format_instructions=parser.get_format_instructions(),\n",
                ")\n",
                "\n",
                "LLM_ID = None\n",
                "structured_joke = structured_prompt | get_llm(llm_id=LLM_ID) | parser\n",
                "\n",
                "r = structured_joke.invoke({\"topic\": \"cat\"})\n",
                "print(r)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from devtools import debug\n",
                "\n",
                "debug(r)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(parser.get_format_instructions())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(structured_prompt)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# You can have a look at the generated prompt:\n",
                "print(structured_prompt.invoke({\"topic\": \"cat\"}).messages[0].content)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Method 2: Function Calling\n",
                "\n",
                "This approach uses the model's native structured output capabilities via:\n",
                "\n",
                "1. The `with_structured_output()` method\n",
                "2. The same Pydantic schema for type safety\n",
                "3. Fewer prompt instructions needed\n",
                "\n",
                "This typically works better with models that support function calling (like GPT-4)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "prompt = \"tell me  a joke on {topic}\"\n",
                "\n",
                "# MODEL = None\n",
                "MODEL = \"gpt_4omini_edenai\"  # better with Azure\n",
                "chain = def_prompt(prompt) | get_llm(llm_id=MODEL).with_structured_output(Joke)\n",
                "print(chain.invoke({\"topic\": \"cat\"}))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Assignment: Joke Rating System\n",
                "\n",
                "Let's extend our structured output by:\n",
                "\n",
                "1. Creating an Enum to represent joke ratings\n",
                "2. Modifying our Joke class to include this rating\n",
                "3. Using LangChain's Enum output parser\n",
                "\n",
                "Reference: [LangChain Enum Output Parser](https://python.langchain.com/v0.1/docs/modules/model_io/output_parsers/types/enum/)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from enum import Enum\n",
                "\n",
                "\n",
                "class JokeRater(Enum):\n",
                "    NOT_SO_GOOD = 0\n",
                "    GOOD = 1\n",
                "    VERY_GOOD = 2"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "genai-blueprint (3.12.3)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}