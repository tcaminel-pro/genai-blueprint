{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Vector Store\n",
                "\n",
                "This notebook demonstrates how to work with vector stores for semantic search and retrieval.\n",
                "\n",
                "We'll cover:\n",
                "- Loading and chunking text documents\n",
                "- Storing document embeddings in different vector databases\n",
                "- Querying and retrieving relevant documents\n",
                "- Using vector stores as configurable LangChain runnables"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from dotenv import load_dotenv\n",
                "from rich import print\n",
                "\n",
                "load_dotenv(verbose=True)\n",
                "\n",
                "%load_ext autoreload\n",
                "%autoreload 2\n",
                "\n",
                "import sys\n",
                "\n",
                "sys.path.append(\".\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Split the text into chunks\n",
                "\n",
                "First we load a text document and split it into smaller chunks for processing:\n",
                "\n",
                "- Uses LangChain's `TextLoader` to load the file\n",
                "- Applies `RecursiveCharacterTextSplitter` to break text into 2000-character chunks\n",
                "- No overlap between chunks is configured"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from genai_tk.core.embeddings_factory import EmbeddingsFactory\n",
                "from genai_tk.core.embeddings_store import VECTOR_STORE_ENGINE, EmbeddingsStore\n",
                "from langchain_community.document_loaders import TextLoader\n",
                "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
                "\n",
                "loader = TextLoader(\"use_case_data/other/state_of_the_union.txt\")\n",
                "documents = loader.load()\n",
                "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=0)\n",
                "texts = text_splitter.split_documents(documents)\n",
                "print(texts)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Store document embeddings in a vector database\n",
                "\n",
                "We use our `EmbeddingsStore` to:\n",
                "\n",
                "1. Select a vector store backend (default is in-memory)\n",
                "2. Configure the embedding model (default from config)\n",
                "3. Add our document chunks to the store\n",
                "\n",
                "Key benefits of the factory pattern:\n",
                "- Easy switching between vector store implementations\n",
                "- Consistent interface regardless of backend\n",
                "- Centralized configuration management"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "vs_engine: VECTOR_STORE_ENGINE | None = None\n",
                "vs_engine = \"InMemory\"\n",
                "\n",
                "# Other choices (Examples)\n",
                "# vs_engine = \"Chroma_in_memory\"\n",
                "# vs_engine = \"Sklearn\"\n",
                "\n",
                "embeddings_store = EmbeddingsStore(\n",
                "    id=vs_engine,\n",
                "    table_name_prefix=\"name\",\n",
                "    embeddings_factory=EmbeddingsFactory(),\n",
                ")\n",
                "\n",
                "print(embeddings_store)\n",
                "\n",
                "db = embeddings_store.get()\n",
                "db.add_documents(texts)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Test semantic search queries\n",
                "\n",
                "We'll search for content related to:\n",
                "1. \"What did the president say about Ketanji Brown Jackson\" (English)\n",
                "2. \"Qu'as dit le président sur Ketanji Brown Jackson\" (French)\n",
                "\n",
                "This demonstrates:\n",
                "- The vector store finds relevant content regardless of query language\n",
                "- Semantic similarity works across languages when using multilingual embeddings"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "query = \"What did the president say about Ketanji Brown Jackson\"\n",
                "docs = db.similarity_search(query, k=3)\n",
                "print(docs)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "query = \"Qu'as dit le président sur Ketanji Brown Jackson\"\n",
                "docs = db.similarity_search(query, k=3)\n",
                "print(docs)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Vector Store as Runnable\n",
                "\n",
                "LangChain's `as_retriever()` converts the vector store into a runnable component that can:\n",
                "\n",
                "- Be chained with other LangChain components\n",
                "- Support streaming and async operations\n",
                "- Be configured with search parameters"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "retriever = db.as_retriever()\n",
                "\n",
                "a = retriever.invoke(query, k=1)\n",
                "print(a)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "genai-blueprint (3.12.3)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}