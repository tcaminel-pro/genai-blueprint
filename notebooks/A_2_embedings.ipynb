{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "\n",
    "This notebook demonstrates how to work with text embeddings using various models. \n",
    "\n",
    "Embeddings are numerical representations of text that capture semantic meaning, allowing us to perform operations like similarity comparison and clustering.\n",
    "\n",
    "We'll explore:\n",
    "- Loading different embedding models\n",
    "- Generating embeddings for text\n",
    "- Calculating similarity between embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Setup environment\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "load_dotenv(verbose=True)\n",
    "assert find_dotenv(), \"no .env file found\"\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "!export PYTHONPATH=$PYTHONPATH:$(pwd)\n",
    "\n",
    "# Force reload modules (add this cell and run it first)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Clear any cached imports\n",
    "import sys\n",
    "\n",
    "if \"genai_tk\" in sys.modules:\n",
    "    del sys.modules[\"genai_tk\"]\n",
    "    del sys.modules[\"genai_tk.core\"]\n",
    "    del sys.modules[\"genai_tk.core.embeddings_factory\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTENCE_1 = \"Tokenization is the process of breaking down a text into individual units.\"\n",
    "SENTENCE_2 = \"Tokens can be words, phrases, or even individual characters.\"\n",
    "SENTENCE_3 = \"LangChain Provide a standardized way to load and process various types of documents\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Available Embedding Models\n",
    "\n",
    "Our system provides a factory pattern for creating different embedding models. \n",
    "\n",
    "The available models are defined in [embeddings.py](../python/ai_core/embeddings.py).\n",
    "\n",
    "Let's list all available models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ada_002_azure', 'ada_002_edenai', 'ada_002_openai', 'artic_22_ollama', 'bge_m3_deepinfra', 'camembert_large_huggingface', 'embeddings_768_fake', 'gte15_base_huggingface', 'gte15_large_huggingface', 'minilm_multilingual_huggingface', 'minilm_v2_huggingface', 'mistral_1024_edenai', 'mxbai_large_ollama', 'qwen3_06b_deepinfra', 'qwen3_4b_deepinfra', 'solon_large_huggingface', 'stella_400_huggingface']\n"
     ]
    }
   ],
   "source": [
    "from genai_tk.core.embeddings_factory import EmbeddingsFactory, get_embeddings\n",
    "\n",
    "# from genai_tk.core.embeddings_factory import EmbeddingsFactory, get_embeddings\n",
    "\n",
    "print(EmbeddingsFactory.known_items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create embeddings for our first sentence using different models.\n",
    "\n",
    "We'll use cosine similarity to compare how similar the embeddings are.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-01 14:52:33.881\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mgenai_tk.core.embeddings_factory\u001b[0m:\u001b[36mget_embeddings\u001b[0m:\u001b[36m357\u001b[0m - \u001b[34m\u001b[1mget embeddings: 'qwen3_06b_deepinfra'\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.011049984022974968, -0.05749082565307617, -0.009234077297151089, -0.09458167850971222, -0.005138628650456667, 0.014449979178607464, -0.0380181260406971, -0.0044238572008907795, -0.07541806995868683, 0.0265817791223526, -0.028436321765184402, -0.06985444575548172, 0.015918157994747162, -0.007727261632680893, -0.03585449233651161, 0.03678176552057266, -0.037709034979343414, -0.006374990567564964, 0.004327266477048397, -0.025345416739583015]...\n",
      "length: 1024\n"
     ]
    }
   ],
   "source": [
    "# Generate Embeddings\n",
    "\n",
    "\n",
    "from langchain_community.utils.math import cosine_similarity\n",
    "\n",
    "# Try different models by uncommenting one:\n",
    "MODEL_ID = \"ada_002_azure\"\n",
    "MODEL_ID = None  # Default\n",
    "embedder = get_embeddings(embeddings_id=MODEL_ID)\n",
    "\n",
    "# or select by tag from a configuration YAML file:\n",
    "# azure_embedder = get_embeddings(embeddings_tag=\"azure\")\n",
    "\n",
    "# Generate embedding for first sentence\n",
    "vector_1 = embedder.embed_documents([SENTENCE_1])\n",
    "print(f\"{vector_1[0][:20]}...\")\n",
    "print(f\"length: {len(vector_1[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare how similar our first sentence is to the other sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.73004097 0.44201278]]\n"
     ]
    }
   ],
   "source": [
    "# Compare Embeddings\n",
    "\n",
    "other_vectors = embedder.embed_documents([SENTENCE_2, SENTENCE_3])\n",
    "\n",
    "result = cosine_similarity(vector_1, other_vectors)\n",
    "print(result)\n",
    "\n",
    "\n",
    "# The output shows the cosine similarity scores between the first sentence and the other two sentences. Scores closer to 1 indicate higher similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vector_1[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment\n",
    "\n",
    "1. Try different sentences and observe how the similarity scores change\n",
    "2. Experiment with different embedding models by changing MODEL_ID\n",
    "3. Explore the [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard) to compare embedding model performance\n",
    "\n",
    "Some things to try:\n",
    "- How do different models handle synonyms?\n",
    "- What happens with very short vs very long sentences?\n",
    "- How do the embedding dimensions differ between models?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai-bp (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
